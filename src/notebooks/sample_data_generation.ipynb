{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from Amazon Reviews'23 Dataset\n",
    "\n",
    "The downloaded Amazon Reviews'23 dataset is a huge dataset with 571 million reviews inside from 34 different product categories. <br>\n",
    "To make more rapid experiments with lower resource requirements (GPU, memory and disk space etc.) and cost, decided to take below actions:<br>\n",
    "* Make a wise down sampling from the huge data to ensure there are large enough samples to have a rating and category agnostic sentiment analysis,<br>\n",
    "* Made some smart feature engineering actions (rather than storing images, just extracted has_image feature and dropped images and categories added by using file names of the reviews with no processing requirement of product meta files) <br> \n",
    "* Build the customized model to benefit from transfer learning by fine-tuning of fundamental LLM models trained with super big data,<br>\n",
    "* Utilize a cloud environment to benefit from flexible and free resources as much as possible\n",
    "\n",
    "## Steps Followed for Sampling and Relevant Tech Stack\n",
    "There were multiple steps to follow after downloading the dataset. Rather than processing all files together which requires huge resources, the processing is executed per file separately and saved the outputs as csv files. Below steps are followed:<br>\n",
    "1. Using Pyspark to open and index each review file per product category as Spark Data Frame by benefiting Spark's parallel processing capabilities<br>\n",
    "2. Generating two new features ('product_category', 'has_image') and dropping one feture column ('images' which contains image urls) <br>\n",
    "3. Taking required number of samples from each rating of each product category and saving as temporary csv files.<br>\n",
    "4. Opening each csv folder with Pandas to get single partition file which is in csv format and removing the rows with more than expected columns (For example record is expected to have 11 columns but there are more or less columns) and the rows with unexpected value types in columns (for example there are boolean type columns with number inside for some rows). <br>\n",
    "5. Merging all cleaned data for each category into a single clean data file and saving it as parquet file for future steps. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col, isnull, size\n",
    "from itertools import count\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings of Sampling Process\n",
    "There are multiple setting to use for above sampling steps\n",
    "* **'target_sample_count':** The total number of samples to get from the large raw dataset **(Step 3)**<br>\n",
    "* **'input_data_directory_path':** The location of downloaded large raw dataset to read files with Spark Session **(Step 1)** <br>\n",
    "* **'input_data_file_extension':** The file format of downloaded large raw dataset files **(Step 1)** <br>\n",
    "* **'output_sampled_data_directory_path':** The location to save sampled temporary data files by using Pyspark **(Step 3)** <br>\n",
    "* **'output_sampled_data_file_extension':** The file format of the temporary sample files **(Step 3)** <br>\n",
    "* **'clean_sampled_data_directory_path':** The location to save the final unified clean data sample file for each product category and rating **(Step 5)**<br>\n",
    "* **'clean_sampled_data_file_extension':** The file format of the final unified clean data sample file **(Step 5)** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sample_count = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_directory_path = \"D:\\\\Datasets\\\\amazon-product-review-2023\\\\\"\n",
    "input_data_file_extension = \".jsonl.gz\"\n",
    "\n",
    "output_sampled_data_directory_path = \"C:\\\\amazon-sampled-dataset\\\\\"\n",
    "output_sampled_data_file_extension = \".csv\"\n",
    "\n",
    "clean_sampled_data_directory_path = \"C:\\\\amazon-sampled-dataset\\\\cleaned-data\\\\\"\n",
    "clean_sampled_data_file_extension = \".parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Session to read files into Spart Data Frame\n",
    "spark = SparkSession.builder.appName(\"Amazon Product Review Sampling App\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\", \"10g\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "        .config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "        .config(\"spark.executor.memory\", \"3g\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The directory 'D:\\Datasets\\amazon-product-review-2023\\' does not exist.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def find_files_with_extension(directory_path, extension):\n",
    "    \"\"\"Find all raw data files in specified format and return as a list\n",
    "\n",
    "    Args:\n",
    "        directory_path (string): The directory path to search for raw data files\n",
    "        extension (string): The file extension to search for (e.g., \".txt\", \".csv\")\n",
    "\n",
    "    Returns:\n",
    "        list: The list of file items\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_items = os.listdir(directory_path)\n",
    "        files = [item for item in all_items if os.path.isfile(os.path.join(directory_path, item)) and item.endswith(extension)]\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{directory_path}' does not exist.\")\n",
    "        return []\n",
    "\n",
    "# To save from product meta files processing, the file names are used as product category information to enrich review data with product categories\n",
    "\n",
    "files = find_files_with_extension(input_data_directory_path, input_data_file_extension)\n",
    "product_categories = [file_name.split(\".\")[0] for file_name in files]\n",
    "print(product_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5882.0\n"
     ]
    }
   ],
   "source": [
    "# The toal number of samples for each rating from each product category is calculated\n",
    "\n",
    "product_category_count = len(product_categories)\n",
    "rating_groups_count = 5\n",
    "\n",
    "# Note: Below // 1 is for flooring the division to integer - a shortcut python operation same with math.floor()\n",
    "samples_per_group = (target_sample_count / (product_category_count * rating_groups_count)) // 1\n",
    "print(samples_per_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: All_Beauty started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Amazon_Fashion started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Appliances started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Arts_Crafts_and_Sewing started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Automotive started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Baby_Products started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Beauty_and_Personal_Care started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Books started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: CDs_and_Vinyl started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Cell_Phones_and_Accessories started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Clothing_Shoes_and_Jewelry started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Digital_Music started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Electronics started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Gift_Cards started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Grocery_and_Gourmet_Food started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Handmade_Products started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Health_and_Household started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Health_and_Personal_Care started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Home_and_Kitchen started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Industrial_and_Scientific started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Kindle_Store started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Magazine_Subscriptions started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Movies_and_TV started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Musical_Instruments started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Office_Products started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Patio_Lawn_and_Garden started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Pet_Supplies started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Software started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Sports_and_Outdoors started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Subscription_Boxes started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Tools_and_Home_Improvement started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Toys_and_Games started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Unknown started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n",
      "Category: Video_Games started to be processed..\n",
      "Rating: 1\n",
      "Rating: 2\n",
      "Rating: 3\n",
      "Rating: 4\n",
      "Rating: 5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For each product category do below steps\n",
    "    1. Read the reviews json file for that category\n",
    "    2. Add a new column 'category' with the category name (extracted from file name)\n",
    "    3. Add a new column 'has_image' which is true if the review has images\n",
    "    4. For each rating (1 to 5) do\n",
    "        a. Calculate the fraction of samples for that rating\n",
    "        b. If the fraction is greater than 1, then set it to 1.0\n",
    "        c. Sample the reviews for that rating with replacement=false and the calculated fraction\n",
    "    5. Reduce the list of sampled dataframes for each rating into a single dataframe\n",
    "    6. Write the sampled dataframe back to file system in csv format with header and overwrite mode\n",
    "\"\"\"\n",
    "for category_name in product_categories:\n",
    "    print(f\"Category: {category_name} started to be processed..\")\n",
    "    df_category_reviews = spark.read.json(input_data_directory_path + category_name + input_data_file_extension)\n",
    "    df_category_reviews = df_category_reviews.withColumn(\"category\", lit(category_name))\n",
    "    df_category_reviews = df_category_reviews.withColumn(\"has_image\", size(col(\"images\")) > 0).drop(\"images\")\n",
    "\n",
    "    sampled_df_list_by_rating = []\n",
    "\n",
    "    for rating in range(1, 6):\n",
    "        print(f\"Rating: {rating}\")\n",
    "        fraction = samples_per_group / df_category_reviews.filter(col(\"rating\") == rating).count()\n",
    "\n",
    "        # For some product categories we do not have enough number of samples for certain ratings, so we take all of them\n",
    "        if fraction > 1:\n",
    "            fraction = 1.0\n",
    "\n",
    "        sampled_df_by_rating = df_category_reviews.filter(col(\"rating\") == rating).sample(withReplacement=False, fraction=fraction, seed=61)\n",
    "        sampled_df_list_by_rating.append(sampled_df_by_rating)    \n",
    "\n",
    "    # below reduce function in functools works in map/reduce approach and traverse all list items until the end \n",
    "    df_sampled_category_reviews  = reduce(lambda df1, df2: df1.union(df2), sampled_df_list_by_rating)\n",
    "\n",
    "    # write the sampled list back to file system as csv format\n",
    "    # coalesce + 1 means generate a single partition when writing, not multiple partitions\n",
    "    df_sampled_category_reviews.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(output_sampled_data_directory_path + category_name + output_sampled_data_file_extension)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinc we generated smaller sampled files in file system, there is no need to work with Spark anymore, so terminating the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All_Beauty', 'Amazon_Fashion', 'Appliances', 'Arts_Crafts_and_Sewing', 'Automotive', 'Baby_Products', 'Beauty_and_Personal_Care', 'Books', 'CDs_and_Vinyl', 'Cell_Phones_and_Accessories', 'Clothing_Shoes_and_Jewelry', 'Digital_Music', 'Electronics', 'Gift_Cards', 'Grocery_and_Gourmet_Food', 'Handmade_Products', 'Health_and_Household', 'Health_and_Personal_Care', 'Home_and_Kitchen', 'Industrial_and_Scientific', 'Kindle_Store', 'Magazine_Subscriptions', 'Movies_and_TV', 'Musical_Instruments', 'Office_Products', 'Patio_Lawn_and_Garden', 'Pet_Supplies', 'Software', 'Sports_and_Outdoors', 'Subscription_Boxes', 'Tools_and_Home_Improvement', 'Toys_and_Games', 'Unknown', 'Video_Games']\n"
     ]
    }
   ],
   "source": [
    "def find_folders_endwith_extension(directory_path, extension):\n",
    "    \"\"\"\n",
    "    Find all folders in the given directory that end with the specified extension.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path to the directory.\n",
    "        extension (str): The extension to search for (e.g., \".txt\").\n",
    "\n",
    "    Returns:\n",
    "        list: A list of folder names that end with the specified extension.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_items = os.listdir(directory_path)\n",
    "        folders = [item for item in all_items if os.path.isdir(os.path.join(directory_path, item)) and item.endswith(extension)]\n",
    "        return folders\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{directory_path}' does not exist.\")\n",
    "        return []\n",
    "\n",
    "folders = find_folders_endwith_extension(output_sampled_data_directory_path, output_sampled_data_file_extension)\n",
    "product_categories = [folder_name.split(\".\")[0] for folder_name in folders]\n",
    "print(product_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sampled_data_of_category(directory_path, extension, category_name):\n",
    "    \"\"\"Inside the directory of product category finds the spark export files in specified format and return the file\n",
    "\n",
    "    Args:\n",
    "        directory_path (string): Path to the directory containing the product category folders\n",
    "        extension (string): The extension of the files to be read (e.g., '.csv', '.json', '.parquet')\n",
    "        category_name (string): The name of the product category folder\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The data frame that contains the data in temporary spark export file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        files = find_files_with_extension(directory_path + category_name + extension, extension)\n",
    "        if len(files) == 0:\n",
    "            return None\n",
    "        \n",
    "        print(directory_path + category_name + extension + \"\\\\\" + files[0])\n",
    "        \n",
    "        # on_bad_lines parameter eliminates the broken rows (such as expected 8 columns but provided 12 columns)\n",
    "        return pd.read_csv(directory_path + category_name + extension + \"\\\\\" + files[0], sep=',', dtype=str, on_bad_lines='skip')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{directory_path + category_name + extension + files[0]}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "def write_sampled_data_to_file(df, file_path, extension):\n",
    "    \"\"\"Writes the data frame into file system\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The data frame to write to file\n",
    "        file_path (string): The path of the data file to write\n",
    "        extension (string): The extension of the data file\n",
    "    \"\"\"\n",
    "    if extension == '.csv':\n",
    "        df.to_csv(file_path, index=False)\n",
    "    elif extension == '.json':\n",
    "        df.to_json(file_path, orient='records')\n",
    "    elif extension == '.parquet':\n",
    "        df.to_parquet(file_path)\n",
    "    else:\n",
    "        print(\"Invalid file extension. Please provide a valid extension (.csv, .json, or .parquet).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, column_types):\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame based on the provided column types.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame to be cleaned.\n",
    "        column_types (dict): A dictionary mapping column names to their expected data types.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The cleaned DataFrame with correct data types.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Function to check if a value matches the expected type\n",
    "    def is_correct_type(value, expected_type):\n",
    "        if expected_type == bool:\n",
    "            return value.lower() in ['true', 'false', '0', '1']\n",
    "        elif expected_type == int:\n",
    "            return value.isdigit()\n",
    "        elif expected_type == float:\n",
    "            try:\n",
    "                float(value)\n",
    "                return True\n",
    "            except ValueError:\n",
    "                return False\n",
    "        elif expected_type == str:\n",
    "            return True  # Assuming all values can be strings\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Identify rows with type mismatches\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    for column, expected_type in column_types.items():\n",
    "        if column in df.columns:\n",
    "            mask &= df[column].apply(lambda x: is_correct_type(str(x), expected_type))\n",
    "\n",
    "    # Keep only the rows that match all type expectations\n",
    "    df_clean = df[mask]\n",
    "\n",
    "    # Convert columns to their proper types\n",
    "    for column, expected_type in column_types.items():\n",
    "        if column in df_clean.columns:\n",
    "            if expected_type == bool:\n",
    "                df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n",
    "            elif expected_type in [int, float]:\n",
    "                df_clean[column] = df_clean[column].astype(expected_type)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "column_types = {\n",
    "    'asin': str,\n",
    "    'helpful_vote': int,\n",
    "    'parent_asin': str,\n",
    "    'rating': float,\n",
    "    'text': str,\n",
    "    'timestamp': str,\n",
    "    'title': str,\n",
    "    'user_id': str,\n",
    "    'verified_purchase': bool,\n",
    "    'category': str,\n",
    "    'has_image': bool\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\amazon-sampled-dataset\\All_Beauty.csv\\part-00000-dd5552d7-8773-4759-bb70-89bb389750fa-c000.csv\n",
      "All_Beauty Original row count: 29703\n",
      "All_Beauty Cleaned row count: 29703\n",
      "All_Beauty Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Amazon_Fashion.csv\\part-00000-c3922f1a-043f-4aa0-85a4-5f3125e4dbd4-c000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon_Fashion Original row count: 29180\n",
      "Amazon_Fashion Cleaned row count: 29179\n",
      "Amazon_Fashion Rows removed: 1\n",
      "C:\\amazon-sampled-dataset\\Appliances.csv\\part-00000-6500a0d7-bfac-4dee-97c8-ef8c23a80658-c000.csv\n",
      "Appliances Original row count: 29070\n",
      "Appliances Cleaned row count: 29070\n",
      "Appliances Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Arts_Crafts_and_Sewing.csv\\part-00000-6a01b8b4-7417-4e8a-b247-5f8d2c8c9676-c000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arts_Crafts_and_Sewing Original row count: 29244\n",
      "Arts_Crafts_and_Sewing Cleaned row count: 29243\n",
      "Arts_Crafts_and_Sewing Rows removed: 1\n",
      "C:\\amazon-sampled-dataset\\Automotive.csv\\part-00000-5d254ddc-cf40-4922-a915-30bcdbe9d6e3-c000.csv\n",
      "Automotive Original row count: 29277\n",
      "Automotive Cleaned row count: 29277\n",
      "Automotive Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Baby_Products.csv\\part-00000-a262cc79-3fc1-4301-b092-9667c828b488-c000.csv\n",
      "Baby_Products Original row count: 29187\n",
      "Baby_Products Cleaned row count: 29187\n",
      "Baby_Products Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Beauty_and_Personal_Care.csv\\part-00000-9ca9abf2-4c2f-4381-9374-7b4dec75d6f2-c000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beauty_and_Personal_Care Original row count: 29468\n",
      "Beauty_and_Personal_Care Cleaned row count: 29467\n",
      "Beauty_and_Personal_Care Rows removed: 1\n",
      "C:\\amazon-sampled-dataset\\Books.csv\\part-00000-31f5ded9-5095-4d28-973c-f7cbabcf3923-c000.csv\n",
      "Books Original row count: 27139\n",
      "Books Cleaned row count: 27139\n",
      "Books Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\CDs_and_Vinyl.csv\\part-00000-2870c220-e0d7-4892-9122-9058a7d24b1e-c000.csv\n",
      "CDs_and_Vinyl Original row count: 25254\n",
      "CDs_and_Vinyl Cleaned row count: 25254\n",
      "CDs_and_Vinyl Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Cell_Phones_and_Accessories.csv\\part-00000-db6e3bd2-386c-492f-83b2-671fc894854a-c000.csv\n",
      "Cell_Phones_and_Accessories Original row count: 29452\n",
      "Cell_Phones_and_Accessories Cleaned row count: 29452\n",
      "Cell_Phones_and_Accessories Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Clothing_Shoes_and_Jewelry.csv\\part-00000-863d6c3f-f7ec-4f56-912a-12af08ac496b-c000.csv\n",
      "Clothing_Shoes_and_Jewelry Original row count: 28875\n",
      "Clothing_Shoes_and_Jewelry Cleaned row count: 28875\n",
      "Clothing_Shoes_and_Jewelry Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Digital_Music.csv\\part-00000-9d743be9-e385-4c53-b2fc-a1021d1811bb-c000.csv\n",
      "Digital_Music Original row count: 24327\n",
      "Digital_Music Cleaned row count: 24327\n",
      "Digital_Music Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Electronics.csv\\part-00000-e26cee9e-7e53-4831-9a3e-25631c8aa09b-c000.csv\n",
      "Electronics Original row count: 28726\n",
      "Electronics Cleaned row count: 28726\n",
      "Electronics Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Gift_Cards.csv\\part-00000-50fdb374-e1d4-4182-bcb2-cd90745b649d-c000.csv\n",
      "Gift_Cards Original row count: 22516\n",
      "Gift_Cards Cleaned row count: 22516\n",
      "Gift_Cards Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Grocery_and_Gourmet_Food.csv\\part-00000-401f54a7-d6cb-406b-a805-33f269a5da3b-c000.csv\n",
      "Grocery_and_Gourmet_Food Original row count: 29224\n",
      "Grocery_and_Gourmet_Food Cleaned row count: 29224\n",
      "Grocery_and_Gourmet_Food Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Handmade_Products.csv\\part-00000-c5a5b280-17fa-4b14-ac8a-0d70848122ed-c000.csv\n",
      "Handmade_Products Original row count: 29598\n",
      "Handmade_Products Cleaned row count: 29598\n",
      "Handmade_Products Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Health_and_Household.csv\\part-00000-3e0ef038-dfc8-4559-91e5-3dfaded4216d-c000.csv\n",
      "Health_and_Household Original row count: 29422\n",
      "Health_and_Household Cleaned row count: 29422\n",
      "Health_and_Household Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Health_and_Personal_Care.csv\\part-00000-b6d08238-5e07-4740-9a51-6e2c2f4dfa03-c000.csv\n",
      "Health_and_Personal_Care Original row count: 29547\n",
      "Health_and_Personal_Care Cleaned row count: 29547\n",
      "Health_and_Personal_Care Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Home_and_Kitchen.csv\\part-00000-281a2ebe-86d4-4e26-9783-c4f83b8fd328-c000.csv\n",
      "Home_and_Kitchen Original row count: 28876\n",
      "Home_and_Kitchen Cleaned row count: 28876\n",
      "Home_and_Kitchen Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Industrial_and_Scientific.csv\\part-00000-99ed82fd-61d8-4feb-8215-a9706bb62fe0-c000.csv\n",
      "Industrial_and_Scientific Original row count: 28945\n",
      "Industrial_and_Scientific Cleaned row count: 28945\n",
      "Industrial_and_Scientific Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Kindle_Store.csv\\part-00000-bc49e5e6-6768-4b69-87bd-25074795cb64-c000.csv\n",
      "Kindle_Store Original row count: 28516\n",
      "Kindle_Store Cleaned row count: 28516\n",
      "Kindle_Store Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Magazine_Subscriptions.csv\\part-00000-f4acab6e-e5dc-41c4-9339-430e3a545d2e-c000.csv\n",
      "Magazine_Subscriptions Original row count: 25486\n",
      "Magazine_Subscriptions Cleaned row count: 25486\n",
      "Magazine_Subscriptions Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Movies_and_TV.csv\\part-00000-1a37dda6-e99f-47f1-a48e-462054f4ca53-c000.csv\n",
      "Movies_and_TV Original row count: 28268\n",
      "Movies_and_TV Cleaned row count: 28268\n",
      "Movies_and_TV Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Musical_Instruments.csv\\part-00000-304e170a-6574-4100-9fb8-599cce6acaac-c000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musical_Instruments Original row count: 28805\n",
      "Musical_Instruments Cleaned row count: 28804\n",
      "Musical_Instruments Rows removed: 1\n",
      "C:\\amazon-sampled-dataset\\Office_Products.csv\\part-00000-d5e87346-cf34-4f2f-b195-cd102e81c7cd-c000.csv\n",
      "Office_Products Original row count: 28982\n",
      "Office_Products Cleaned row count: 28982\n",
      "Office_Products Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Patio_Lawn_and_Garden.csv\\part-00000-07e693b9-b6ab-4942-a01f-d84a59355fed-c000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patio_Lawn_and_Garden Original row count: 29286\n",
      "Patio_Lawn_and_Garden Cleaned row count: 29285\n",
      "Patio_Lawn_and_Garden Rows removed: 1\n",
      "C:\\amazon-sampled-dataset\\Pet_Supplies.csv\\part-00000-c1a4d709-db54-44db-8361-df3f194ced5f-c000.csv\n",
      "Pet_Supplies Original row count: 29474\n",
      "Pet_Supplies Cleaned row count: 29474\n",
      "Pet_Supplies Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Software.csv\\part-00000-8ae68f3e-8eca-4ae2-83b5-e30c1e6b01a7-c000.csv\n",
      "Software Original row count: 29192\n",
      "Software Cleaned row count: 29192\n",
      "Software Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Sports_and_Outdoors.csv\\part-00000-b6f4d328-e0e2-4591-9384-76b18707574d-c000.csv\n",
      "Sports_and_Outdoors Original row count: 29146\n",
      "Sports_and_Outdoors Cleaned row count: 29146\n",
      "Sports_and_Outdoors Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Subscription_Boxes.csv\\part-00000-388df76a-c9f7-4aa3-9a7d-36896b051bcc-c000.csv\n",
      "Subscription_Boxes Original row count: 12930\n",
      "Subscription_Boxes Cleaned row count: 12930\n",
      "Subscription_Boxes Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Tools_and_Home_Improvement.csv\\part-00000-3ea95b14-e8fd-4c87-b6c7-396e9ee73a11-c000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].astype(expected_type)\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n",
      "C:\\Users\\tubac\\AppData\\Local\\Temp\\ipykernel_6176\\74790732.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[column] = df_clean[column].map({'true': True, 'false': False, '0': False, '1': True})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools_and_Home_Improvement Original row count: 29148\n",
      "Tools_and_Home_Improvement Cleaned row count: 29147\n",
      "Tools_and_Home_Improvement Rows removed: 1\n",
      "C:\\amazon-sampled-dataset\\Toys_and_Games.csv\\part-00000-8f98471a-b38c-48e6-83ce-e4b49abae4bc-c000.csv\n",
      "Toys_and_Games Original row count: 29315\n",
      "Toys_and_Games Cleaned row count: 29315\n",
      "Toys_and_Games Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Unknown.csv\\part-00000-f76284ac-b082-40c2-bf14-9b9fb55d3350-c000.csv\n",
      "Unknown Original row count: 28801\n",
      "Unknown Cleaned row count: 28801\n",
      "Unknown Rows removed: 0\n",
      "C:\\amazon-sampled-dataset\\Video_Games.csv\\part-00000-eeb0a89c-bba0-4aa2-903c-002c2c3678a3-c000.csv\n",
      "Video_Games Original row count: 28431\n",
      "Video_Games Cleaned row count: 28431\n",
      "Video_Games Rows removed: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" The below does below operations\n",
    "1. Reads the sampled data of each category\n",
    "2. Cleans the data by removing unwanted columns and rows\n",
    "3. Prints the info about the removed rows\n",
    "4. Appends the cleaned data to a list\n",
    "5. Concatenates all the cleaned data into a single dataframe\n",
    "6. Writes the concatenated data to a file\n",
    "7. Prints the time taken for the entire process\n",
    "\"\"\"\n",
    "cleaned_df_list = []\n",
    "for product_category in product_categories:\n",
    "    df_sample_data = read_sampled_data_of_category(output_sampled_data_directory_path, output_sampled_data_file_extension, product_category)\n",
    "    cleaned_df = clean_data(df_sample_data, column_types)\n",
    "\n",
    "    # Print info about removed rows\n",
    "    print(f\"{product_category} Original row count: {df_sample_data.shape[0]}\")\n",
    "    print(f\"{product_category} Cleaned row count: {cleaned_df.shape[0]}\")\n",
    "    print(f\"{product_category} Rows removed: {df_sample_data.shape[0] - cleaned_df.shape[0]}\")\n",
    "\n",
    "    cleaned_df_list.append(cleaned_df)\n",
    "\n",
    "cleaned_all_data = pd.concat(cleaned_df_list, ignore_index=True)\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "write_sampled_data_to_file(cleaned_all_data, clean_sampled_data_directory_path + \"sample_amazon_product_review_data_\" + current_datetime + clean_sampled_data_file_extension, clean_sampled_data_file_extension)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_conda_sentiment_kernel_1.0",
   "language": "python",
   "name": "env_conda_sentiment"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
